{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 16 - redução de dimensionalidade\n",
    "\n",
    "Na aula de hoje, vamos explorar os seguintes tópicos em Python:\n",
    "\n",
    "- 1) PCA\n",
    "- 2) PCA na Pipeline\n",
    "- 3) A matemática do PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T14:59:22.306630Z",
     "start_time": "2022-03-07T14:59:18.038145Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "PCA (principal component analysis) é uma técnica **não supervisionada** que determina **as direções de máxima variância** no espaço de features, de modo que **as direções são ortogonais entre si**.\n",
    "\n",
    "Ou seja,\n",
    "\n",
    "> A primeira componente principal é a direção que maximiza a variáncia dos dados projetados em sua direção; A segunda componente principal é uma direção ortogonal à primeira que também maximiza a variância nesta direção, e assim sucessivamente.\n",
    "\n",
    "<img src=https://austingwalters.com/wp-content/uploads/2014/11/gaussDist-labeled.png width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É importante lembrar que:\n",
    "\n",
    "- Se o espaço original tem $n$ features, é possível construir $n$ componentes principais;<br><br>\n",
    "- Cada componente principal é uma **combinação linear das features originais**;<br><br>\n",
    "- As componentes principais são ortogonais entre si;<br><br>\n",
    "- As componentes principais são linearmente descorrelacionadas;<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É muito comum que PCA seja utilizada como uma técnica de **redução de dimensionalidade**, isto porque, **no espaço transformado de componentes principais**, é comum que grande parte da variância completa dos dados esteja contida nas $L$ primeiras componentes principais ($L < n$).\n",
    "\n",
    "Sendo assim, ao invés de utilizar o espaço de features completo (de dimensão $n$) para a construção de modelos, podemos utilizar apenas $L$ dimensões **do espaço de features de componentes principais**, reduzindo, assim, a dimensionalidade do espaço de features efetivo em que os estimadores irão trabalhar.\n",
    "\n",
    "Os objetivos desta redução podem ser:\n",
    "\n",
    "- Evitar a maldição da dimensionalidade;\n",
    "- Treinar modelos mais rápido (de maneira mais eficiente), afinal, há menos dados;\n",
    "- Fornecer dados sem qualquer colinearidade (há estimadores que assumem não-colinearidade!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) O que é cada PC?\n",
    "\n",
    "Como dissemos acima, cada componente principal é uma **combinação linear das features originais**.\n",
    "\n",
    "Nesse sentido, podemos enxergar o PCA como um método que constrói um novo espaço de features, de mesma dimensão que o espaço original:\n",
    "\n",
    "<img src=https://miro.medium.com/max/1200/1*V3JWBvxB92Uo116Bpxa3Tw.png width=700>\n",
    "\n",
    "O ponto é que o espaço de features construído pelo PCA é muito particular: as novas features são ortogonais entre si, e tais que maximizam a variância dos dados em cada sub-espaço de projeção, como dissemos acima.\n",
    "\n",
    "Assim, na prática, seja um espaço de input $\\mathcal{X}$ de $n$ dimensões, tal que cada vetor de features $\\vec{x} \\in \\mathcal{X}$ é dado por:\n",
    "\n",
    "$ \\vec{x} = \\left ( x_1, x_2, x_3, \\cdots , x_n \\right) $\n",
    "\n",
    "Após a aplicação do PCA, estaremos no espaço de componentes principais, tal que um vetor de features neste espaço, $\\vec{x}_{PC}$, será dado por:\n",
    "\n",
    "$ \\vec{x}_{PC} = \\left ( PC_1, PC_2, PC_3, \\cdots , PC_n \\right) $\n",
    "\n",
    "Onde cada componente principal é uma combinação linear das features originais, isto é, \n",
    "\n",
    "$ PC_i = \\sum_{k=1}^n \\alpha_{i, k} x_k = \\alpha_{i, 1} x_1 +  \\alpha_{i, 2} x_2 + \\cdots +  \\alpha_{i, n} x_n$\n",
    "\n",
    "Note que cada componente principal $i$ tem $n$ coeficientes diferentes $\\alpha_{i, k}$, que acompanham cada uma das features $k$ do espaço original.\n",
    "\n",
    "Para entender melhor, vamos tomar um dataset bem simples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width    species\n",
       "0             5.1          3.5           1.4          0.2     setosa\n",
       "1             4.9          3.0           1.4          0.2     setosa\n",
       "2             4.7          3.2           1.3          0.2     setosa\n",
       "3             4.6          3.1           1.5          0.2     setosa\n",
       "4             5.0          3.6           1.4          0.2     setosa\n",
       "..            ...          ...           ...          ...        ...\n",
       "145           6.7          3.0           5.2          2.3  virginica\n",
       "146           6.3          2.5           5.0          1.9  virginica\n",
       "147           6.5          3.0           5.2          2.0  virginica\n",
       "148           6.2          3.4           5.4          2.3  virginica\n",
       "149           5.9          3.0           5.1          1.8  virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../datasets/iris.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('species', axis=1)\n",
    "y = df['species']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No caso do dataset iris, temos 4 features, de modo que o espaço original tem 4 dimensões, e um vetor de features é:\n",
    "\n",
    "$ \\vec{x} = (x_1, x_2, x_3, x_4)$.\n",
    "\n",
    "Sendo que:\n",
    "\n",
    "- $x_1$ : 'sepal_length', \n",
    "- $x_2$ : 'sepal_width', \n",
    "- $x_3$ : 'petal_length', \n",
    "- $x_4$ : 'petal_width'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, esperamos que após o PCA, o espaço de componentes principais também tenha 4 dimensões, isto é, \n",
    "\n",
    "$ \\vec{x}_{PC} = \\left ( PC_1, PC_2, PC_3, PC_4 \\right) $\n",
    "\n",
    "E as componentes principais serão: \n",
    "\n",
    "$ PC_1 = \\alpha_{1, 1} x_1 + \\alpha_{1, 2} x_2 + \\alpha_{1, 3} x_3 + \\alpha_{1, 4} x_4$\n",
    "\n",
    "$ PC_2 = \\alpha_{2, 1} x_1 + \\alpha_{2, 2} x_2 + \\alpha_{2, 3} x_3 + \\alpha_{2, 4} x_4$\n",
    "\n",
    "$ PC_3 = \\alpha_{3, 1} x_1 + \\alpha_{3, 2} x_2 + \\alpha_{3, 3} x_3 + \\alpha_{3, 4} x_4$\n",
    "\n",
    "$ PC_4 = \\alpha_{4, 1} x_1 + \\alpha_{4, 2} x_2 + \\alpha_{4, 3} x_3 + \\alpha_{4, 4} x_4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora aplicar o PCA para calcular as componentes principais!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas antes, um ponto muito importante: **devemos escalar as features originais com o StandardScaler antes de aplicarmos o PCA**!\n",
    "\n",
    "Isso porque o método matemático recebe dados com média 0 e desvio padrão 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train_ss = std.transform(X_train)\n",
    "X_test_ss = std.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora sim!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(random_state=42).fit(X_train_ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o PCA fitado, temos alguns atributos importantes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.n_features_in_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O atributo `components_` dá exatamente os coeficientes $\\alpha_{i, k}$ que determinan as combinações lineares de cada componente principal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.52679335, -0.25307206,  0.58186918,  0.56557189],\n",
       "       [ 0.34813945,  0.93470791,  0.02689438,  0.06630793],\n",
       "       [-0.72745724,  0.22196252,  0.1386138 ,  0.6342908 ],\n",
       "       [ 0.26850263, -0.11405393, -0.80093144,  0.52288323]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou seja,\n",
    "\n",
    "$PC_1 = 0.527x_1 -0.253x_2 + 0.582x_3 + 0.566x_4$\n",
    "\n",
    "$PC_2 = 0.348x_1 + 0.935x_2 + 0.027x_3 + 0.066x_4$\n",
    "\n",
    "$PC_3 = -0.727x_1 + 0.222x_2 + 0.139x_3 + 0.634x_4$\n",
    "\n",
    "$PC_4 = 0.269x_1 -0.114x_2 -0.801x_3 + 0.523x_4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembrando que:\n",
    "\n",
    "- $x_1$ : 'sepal_length', \n",
    "- $x_2$ : 'sepal_width', \n",
    "- $x_3$ : 'petal_length', \n",
    "- $x_4$ : 'petal_width'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Há alguns outros atributos muito importantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72677234, 0.23066667, 0.03781901, 0.00474197])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72677234, 0.95743901, 0.99525803, 1.        ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_.cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos plotar os resultados acima:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 4 artists>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = [f'PC{i+1}' for i in range(pca.n_components_)]\n",
    "\n",
    "plt.bar(labels, pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dos resultados, é fácil ver que com apenas 2 componentes principais, capturamos 95% da variância dos dados!\n",
    "\n",
    "Assim, caso queiramos reduzir o número de dimensões de $n=4$ para $L=2$, temos a garantia de que grande parte da variabilidade estará contida nas duas componentes principais que usaremos para modelagem!\n",
    "\n",
    "> Veja que a escolha de $L < n$ componentes principais pode ser visto como um procedimento de **feature selection**, mas feito **no espaço de componentes principais!**\n",
    "> Como não são as features originais que são selecionadas, é comum nos referirmos a este procedimento como **redução de dimensionalidade** (e a dimensionalidade que é reduzida é do espaço de componentes principais!)\n",
    "\n",
    "De fato, note que **todas as 4 features originais** estão presentes em cada uma das PCs, como termos da combinação linear que define cada PC. E, naturalmente, todas as 4 estão presentes nas $L$ primeiras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso queiramos construir modelos no espaço de componentes principais, é importante que incorporemos o PCA em nossa pipeline! \n",
    "\n",
    "Vamos ver na prática!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) PCA na Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T20:21:07.483704Z",
     "start_time": "2022-03-07T20:21:07.476726Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T20:21:11.941383Z",
     "start_time": "2022-03-07T20:21:11.932368Z"
    }
   },
   "outputs": [],
   "source": [
    "def metricas_classificacao(estimator, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    # ============================================\n",
    "\n",
    "    print(\"\\nMétricas de avaliação de treino:\")\n",
    "\n",
    "    y_pred_train = estimator.predict(X_train)\n",
    "\n",
    "    print(confusion_matrix(y_train, y_pred_train))\n",
    "\n",
    "    ConfusionMatrixDisplay.from_predictions(y_train, y_pred_train)\n",
    "    plt.show()\n",
    "\n",
    "    print(classification_report(y_train, y_pred_train))\n",
    "\n",
    "    # ============================================\n",
    "\n",
    "    print(\"\\nMétricas de avaliação de teste:\")\n",
    "\n",
    "    y_pred_test = estimator.predict(X_test)\n",
    "\n",
    "    print(confusion_matrix(y_test, y_pred_test))\n",
    "\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test, y_pred_test)\n",
    "    plt.show()\n",
    "\n",
    "    print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________\n",
    "\n",
    "Vamos analisar um outro dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos treinar modelos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_no_pca = Pipeline([('lr')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) A matemática do PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Adaptado do notebook do Walisson!)\n",
    "\n",
    "__________\n",
    "\n",
    "Considere a matriz de features $X_{N \\times n}$\n",
    "\n",
    "> $N$ linhas (observações), cada uma caracterizada por $n$ features no espaço original.\n",
    "\n",
    "Cada observação $i$ é caracterizada pelo vetor de features $\\vec{x}_i = (x_{i1}, x_{i2}, \\cdots, x_{in}$), que são as linhas da matriz de features:\n",
    "\n",
    "$$\n",
    "  X = \\left [ \\begin{array}{ccccc}\n",
    "x_{11} & x_{12} & x_{13} & \\cdots & x_{1n}\\\\ \n",
    "x_{21} & x_{22} & x_{23} & \\cdots & x_{2n}\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\cdots & \\vdots  \\\\\n",
    "x_{N1} & x_{N2} & x_{N3} & \\cdots & x_{Nn}\\\\ \n",
    "   \\end{array} \\right ] \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 1 - standardização\n",
    "\n",
    "Para dados em que as features originais $x_j$ estão em escalas diferentes, é necessário escalar os dados para que eles tenham média 0 e desvio padrão 1 (ou seja, usamos o `StandardScaler`). \n",
    "\n",
    "Isso porque os componentes são influenciados pela escala das variáveis, justamente porque as matrizes de covariâncias, $\\Sigma$ ou $\\hat{\\Sigma} = S$, são sensíveis à escala de um par de variáveis. \n",
    "\n",
    "Considere:\n",
    "\n",
    "- $\\bar{x}_j$ a média da variável $x_j$; \n",
    "- $s(x_j)$ o desvio padrão de $x_j$; \n",
    "\n",
    "Sendo $i = 1, 2,3,4,\\cdots, N$ e $j = 1, 2,3,4,\\cdots, n$.\n",
    "\n",
    "Com isso, a padronização pode ser realizada por meio da equação abaixo: \n",
    "\n",
    "- Média 0 e desvio padrão 1: \n",
    "\n",
    "$$ \\tilde{x}_{ij}= \\frac{x_{ij}-\\bar{x_j}}{s(X_j)} $$ \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 2 - cálculo da matriz de covariância\n",
    "\n",
    "Calcular a matriz de **covariância**/**correlação**, que são dadas por:\n",
    "\n",
    "$$\n",
    "  S = \\left [ \\begin{array}{ccccc}\n",
    "\\hat{Var}(x_1) & \\hat{Cov}(x_1x_2) & \\hat{Cov}(x_1x_3) & \\cdots & \\hat{Cov}(x_1x_n)\\\\ \n",
    "\\hat{Cov}(x_2x_1) &\\hat{Var}(x_2)& \\hat{Cov}(x_2x_3) & \\cdots & \\hat{Cov}(x_2x_n)\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\cdots & \\vdots  \\\\\n",
    "\\hat{Cov}(x_nx_1) & \\hat{Cov}(x_nx_2)  & \\hat{Cov}(x_nx_3)  & \\cdots & \\hat{Var}(x_n)\\\\ \n",
    "   \\end{array} \\right ] \n",
    "$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$\n",
    "  R = \\left [ \\begin{array}{ccccc}\n",
    "1 & r(x_1x_2) & r(x_1x_3) & \\cdots & r(x_1x_n)\\\\ \n",
    "r(x_2x_1) & 1 & r(x_2x_3) & \\cdots & r(x_2x_n)\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\cdots & \\vdots  \\\\\n",
    "r(x_nx_1) & r(x_nx_2)  & r(x_nx_3)  & \\cdots & 1\\\\ \n",
    "   \\end{array} \\right ] \n",
    "$$\n",
    "\n",
    "Em que:\n",
    "\n",
    "$$\n",
    " \\begin{array}{ccc}\n",
    "\\hat{Var}(x_j) = \\frac{\\sum_{i=1}^{N}(x_{ij}-\\bar{x}_j)}{N-1}, & \n",
    "\\hat{Cov}(x_{j1},x_{j2}) = \\frac{\\sum_{i=1}^N(x_{ij1}-\\bar{x_{j1}})(x_{ij2}-\\bar{x_{j2}})}{N-1}, &\n",
    "r(x_{j1},x_{j2}) = \\frac{\\hat{Cov}(x_{j1},x_{j2})}{S_{xj1}S_{xj2}}\n",
    "   \\end{array} \n",
    "$$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 3 - determinação de autovalores e autovetores\n",
    "\n",
    "As componentes principais são determinadas através da equação característica (equação de autovalores) da matriz S ou R:\n",
    "\n",
    "$$det[R - \\lambda I]= 0 $$\n",
    "\n",
    "Em que $I$ é a matriz identidade de dimensão $n \\times n $. \n",
    "\n",
    "Se R ou S tem posto completo igual a $n$, então $det[R - \\lambda I]= 0$, que pode ser reescrito como $\\mid R - \\lambda I \\mid = 0$, terá $n$ soluções. Lembrando que ter posto completo significa que nenhuma coluna é combinação linear de outra.\n",
    "\n",
    "Considere que $\\lambda_1,\\lambda_2,\\lambda_3, \\cdots, \\lambda_n$ sejam as raízes da equação característica de R ou S, então temos que  $\\lambda_1 > \\lambda_2 > \\lambda_3 > \\cdots, \\lambda_n$. \n",
    "\n",
    "Chamamos $\\lambda_i$ de **autovalores**. \n",
    "\n",
    "Além disso, para cada autovalor há um **autovetor** $\\tilde{a}_i$ associado, $\n",
    "  \\tilde{a}_i = \\left [ \\begin{array}{c}\n",
    "a_{i1}\\\\ \n",
    "a_{i2}\\\\ \n",
    "\\vdots \\\\\n",
    "a_{ip} \\\\ \n",
    "   \\end{array} \\right ] \n",
    "$\n",
    "\n",
    "O cálculo do autovetor $\\tilde{a}_i$, pode ser realizado considerando a seguinte propriedade:\n",
    "\n",
    "$$ R\\tilde{a}_i =  \\lambda_i \\tilde{a}_i $$\n",
    "\n",
    "O autovetor deve ser normalizado, isso é,\n",
    "\n",
    "$$ a_i = \\frac{\\tilde{a}_i }{\\mid \\tilde{a}_i  \\mid}$$\n",
    "\n",
    "Desta maneira, as componentes do vetor são tais que sua norma L2 é igual a 1.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 4 - cálculo das componentes principais\n",
    "\n",
    "O cálculo da i-ésima componente principal é dado por:\n",
    "\n",
    "$$PC_i = a_{i1}x_1 + a_{i2}x_2 + a_{i3}x_3 + \\cdots + a_{in}x_n $$\n",
    "\n",
    "em que $a_{i1}$ são as componetes do autovetor $a_i$ associado ao autovalor $\\lambda_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "\n",
    "Operacionalmente, o PCA se apoia em um procedimento matemático denominado **Singular Value Decomposition (SVD)**, que é uma forma de decompor qualquer matriz não quadrada $M_{m\\times n}$,\n",
    "\n",
    "$$M_{m\\times n} = U_{m \\times m}\\Sigma_{m \\times n}V_{n \\times n}^{\\dagger}$$\n",
    "\n",
    "- onde $U$ é uma matriz unitária $m\\times m$ real ou complexa;\n",
    "\n",
    "- $\\Sigma$ é uma matriz retangular diagonal $m\\times n$ com números reais não-negativos na diagonal;\n",
    "\n",
    "- e $V^{\\dagger}$ (a conjugada transposta de $V$) é uma matriz unitária $n\\times n$ real ou complexa. \n",
    "\n",
    "Os valores de $\\Sigma$ são os chamados valores singulares de $M$. As $m$ colunas de $U$ e as $n$ colunas de $V$ são os chamados vetores singulares à esquerda e vetores singulares à direita de $A$, respetivamente.\n",
    "\n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Singular_value_decomposition_visualisation.svg/800px-Singular_value_decomposition_visualisation.svg.png width=300>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
