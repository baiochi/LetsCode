{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3379c566",
   "metadata": {},
   "source": [
    "# Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d16a7c0",
   "metadata": {},
   "source": [
    "## Detecção de objetos\n",
    "\n",
    "Há um grande interesse de utilizar técnicas de machine learning para detecção de objetos. Rastreamento de objetos em tempo real (real-time object tracking).\n",
    "\n",
    "Há diversas aplicações como:\n",
    "- Realidade aumentada\n",
    "    - https://www.youtube.com/watch?v=tJbtp1Cv1gY\n",
    "    - https://www.youtube.com/watch?v=aWI39cCNo7I\n",
    "    \n",
    "- Criação de filtros (estilo snapchat e TikTok)\n",
    "- Detecção de poses:\n",
    "    - [Conseguimos controlar um avatar de acordo com os nosso movimentos] (https://www.youtube.com/watch?v=8Va3_jwYOJU)\n",
    "    - [Melhorar a performance de atletas] (https://www.youtube.com/watch?v=-LkMOGvbn_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f27bbe",
   "metadata": {},
   "source": [
    "## MediaPipe\n",
    "\n",
    "Modulo desenvolvido pela Google. Consiste em um módulo de detecção.  \n",
    "Facilidade de implementar soluções sofisticadas de forma simples e ágil.  \n",
    "[Diversas soluções](https://google.github.io/mediapipe/)\n",
    "- Detecção de faces\n",
    "- Poses\n",
    "- Detecção de mãos\n",
    "- E muito mais\n",
    "\n",
    "[Projeto Github](https://github.com/google/mediapipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcb5226",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Install and load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc016ef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!pip install opencv-python\n",
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c49c61b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T00:27:58.107604Z",
     "start_time": "2022-05-03T00:27:58.101105Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Data and math operations\n",
    "import re\n",
    "from math import hypot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Image visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Audio Speech\n",
    "import pyttsx3\n",
    "\n",
    "# For opening images via URL\n",
    "from PIL.Image import open as open_image\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Display images in Jupyter Notebook\n",
    "from IPython.display import Image\n",
    "\n",
    "# macOS alternative to pycaw\n",
    "import osascript\n",
    "\n",
    "# Control the flow of time\n",
    "import time\n",
    "\n",
    "# For docstrings\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c39f8b",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Defines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98efd702",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T23:59:58.575757Z",
     "start_time": "2022-05-02T23:59:58.562477Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# mediapipe instances\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Landmark styles\n",
    "HAND_CONNECTIONS = mp_hands.HAND_CONNECTIONS\n",
    "DEFAULT_LANDMARK_POINTS = mp_drawing_styles.get_default_hand_landmarks_style()\n",
    "DEFAULT_LANDMARK_CONNECTIONS = mp_drawing_styles.get_default_hand_connections_style()\n",
    "BASIC_LANDMARK_POINTS_SPEC = mp_drawing.DrawingSpec(color=(22,150,210), thickness=2, circle_radius=2)\n",
    "BASIC_LANDMARK_CONNECTION_SPEC = mp_drawing.DrawingSpec(color=(253,191,17), thickness=1, circle_radius=2)\n",
    "\n",
    "# Mapping of handpoints\n",
    "_hand_point_names = [ \n",
    "                     ['THUMB_'         + i for i in ['CMC','MCP','IP','TIP']],\n",
    "                     ['INDEX_FINGER_'  + i for i in ['MCP','PIP','DIP','TIP']],\n",
    "                     ['MIDDLE_FINGER_' + i for i in ['MCP','PIP','DIP','TIP']],\n",
    "                     ['RING_FINGER_'   + i for i in ['MCP','PIP','DIP','TIP']],\n",
    "                     ['PINKY_'         + i for i in ['MCP','PIP','DIP','TIP']]\n",
    "                    ]\n",
    "_hand_point_names  = ['WRIST'] + [value for sublist in _hand_point_names for value in sublist]\n",
    "\n",
    "HAND_POINT_MAPPING = {index:_hand_point_names[index] for index in range(21)}\n",
    "\n",
    "# Terminal ASCII colors\n",
    "WHITE = '\\033[39m'\n",
    "CYAN = '\\033[36m'\n",
    "GREEN = '\\033[32m'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca3e3a2",
   "metadata": {},
   "source": [
    "# Custom functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cd89c3",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Draw landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbc3d0ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T00:00:05.696634Z",
     "start_time": "2022-05-03T00:00:05.690845Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Draw landmarks in image with mediapipe default style\n",
    "def draw_landmarks(image, landmarks, connections) -> None:\n",
    "    mp_drawing.draw_landmarks(image, \n",
    "                landmarks,\n",
    "                connections,\n",
    "                DEFAULT_LANDMARK_POINTS,\n",
    "                DEFAULT_LANDMARK_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ed6ae6",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Map hand positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fbf7612",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T00:00:07.530111Z",
     "start_time": "2022-05-03T00:00:07.522767Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Use points coordinates to map hand positions\n",
    "def map_hand_coord(hand_position:list) -> dict():\n",
    "    _hand_coord = [(i[1], i[2]) for i in hand_position]\n",
    "    return {HAND_POINT_MAPPING[index]:coord for index, coord in enumerate(_hand_coord)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c70b2c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Get current hand in loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04fab674",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T00:00:11.500607Z",
     "start_time": "2022-05-03T00:00:11.495385Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Extract str value in mp_hands_results.multi_handedness object\n",
    "get_current_hand = lambda multi_handedness: re.findall('\"([^\"]*)\"', str(multi_handedness))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfc4f2a",
   "metadata": {},
   "source": [
    "## Get hand coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10565a75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T00:00:13.022452Z",
     "start_time": "2022-05-03T00:00:13.015452Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_hand_position(image, hand_landmarks):\n",
    "    '''\n",
    "    Get hand points coordinates in image from a landmark object.\n",
    "    Parameters:\n",
    "    @ image (numpy.ndarray): coordinates of point 1\n",
    "    @ hand_landmarks (mediapipe.framework.formats.landmark_pb2.NormalizedLandmarkList): hand landmark object to extract results\n",
    "    Returns:\n",
    "        list: coordinates for each hand point\n",
    "    '''\n",
    "    height, width, channel = image.shape \n",
    "    landmark_results = []\n",
    "    \n",
    "    for _id, landmark in enumerate(hand_landmarks.landmark):\n",
    "        x_coord, y_coord = int(landmark.x * width), int(landmark.y * height)\n",
    "        landmark_results.append([_id, x_coord, y_coord]\n",
    "        \n",
    "    return landmark_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63a7236",
   "metadata": {},
   "source": [
    "## Calculate distance between two points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd0bbc74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T00:00:17.181310Z",
     "start_time": "2022-05-03T00:00:17.174051Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_points_distance(coord_1, coord_2, draw_points=None):\n",
    "    '''\n",
    "    Calculate distance with `hypot` between two hand points.\n",
    "    Parameters:\n",
    "    @ coord_1 (tuple): coordinates of point 1\n",
    "    @ coord_2 (tuple): coordinates of point 2\n",
    "    @ draw_points (numpy.ndarray): image array to draw annotations\n",
    "    Returns:\n",
    "        float: distance between the two points\n",
    "    '''\n",
    "    if draw_points:\n",
    "        # Color points\n",
    "        cv2.circle(draw_points, coord_1, 10, (0,200,235), cv2.FILLED)\n",
    "        cv2.circle(draw_points, coord_2, 10, (0,200,235), cv2.FILLED)\n",
    "        # Create distance line\n",
    "        cv2.line(draw_points, coord_1, coord_2, (198,138,9), 2) \n",
    "    \n",
    "    x_1, y_1 = coord_1\n",
    "    x_2, y_2 = coord_2\n",
    "    \n",
    "    # Calculate distance with hypotenuse\n",
    "    return hypot(x_2 - x_1, y_2 - y_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e1d278",
   "metadata": {},
   "source": [
    "# Webcam Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a445bdad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T00:29:37.300513Z",
     "start_time": "2022-05-03T00:29:26.672935Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Webcam input\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "# Create mediapipe Hands object\n",
    "hands = mp_hands.Hands(model_complexity=0, min_detection_confidence=0.8, min_tracking_confidence=0.9)\n",
    "\n",
    "# Run webcam\n",
    "while cam.isOpened():\n",
    "    \n",
    "    # Read image frame\n",
    "    success, image = cam.read()\n",
    "    if not success:\n",
    "        print(\"Ignoring empty camera frame.\")\n",
    "        continue\n",
    "        \n",
    "    # Improve performance = False\n",
    "    image.flags.writeable = True\n",
    "    \n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    image = cv2.flip(image, 1)\n",
    "    \n",
    "    # Get image dimensions\n",
    "    h, w, _ = image.shape\n",
    "    \n",
    "    # Process image in RGB color scale\n",
    "    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    # Create hand map\n",
    "    hand_map = {}\n",
    "    \n",
    "    # Compute data for any Hand(s) found\n",
    "    if results.multi_hand_landmarks:\n",
    "        for index, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "            \n",
    "            # Draw the hand points and connections on the image\n",
    "            draw_landmarks(image, hand_landmarks, HAND_CONNECTIONS)\n",
    "            \n",
    "            # Get current hand points coordinates\n",
    "            current_hand = get_current_hand(results.multi_handedness[index])\n",
    "            hand_position = find_hand_position(image, hand_landmarks)\n",
    "            hand_map[current_hand] = map_hand_coord(hand_position)\n",
    "\n",
    "            # Change volume\n",
    "#             if 'Left' in hand_map.keys():\n",
    "#                 # Select fingers\n",
    "#                 finger_1 = hand_map['Left']['THUMB_TIP']\n",
    "#                 finger_2 = hand_map['Left']['INDEX_FINGER_TIP']\n",
    "#                 # Calculate distante and draw in frame\n",
    "#                 finger_distance = get_points_distance(finger_1, finger_2, draw_points=True)\n",
    "#                 # Bar position\n",
    "#                 tx, ty = int(w*0.05), int(h*0.15) # top    x, y coord\n",
    "#                 bx, by = int(w*0.08), int(h*0.35) # bottom x, y coord\n",
    "#                 hand_screen_range = [int(h*0.05), int(h*0.40)] # 3 - 40 % of screen height\n",
    "#                 volume_range = [0, 100]  # System volume range\n",
    "#                 bar_range = [by, ty]     # Bar full range in screen\n",
    "#                 # True value for volume level\n",
    "#                 volume_level = np.interp(finger_distance, hand_screen_range, volume_range)\n",
    "#                 bar_level = np.interp(volume_level, volume_range, bar_range)\n",
    "                \n",
    "#                 # Draw volume bar\n",
    "#                 ## Empty bar (0,200,235) (198,138,9)\n",
    "#                 cv2.rectangle(image, (tx, ty), (bx, by),\n",
    "#                               (20, 20, 20), -1)\n",
    "#                 ## Filling bar\n",
    "#                 cv2.rectangle(image,\n",
    "#                               (tx, int(bar_level)),\n",
    "#                               (bx, by),\n",
    "#                               (240, 240, 240), cv2.FILLED)\n",
    "#                 ## Volume text\n",
    "#                 cv2.putText(image,\n",
    "#                             f'{int(volume_level)}%', \n",
    "#                             (tx-10, ty-10), \n",
    "#                             cv2.FONT_ITALIC,1 , (240, 240, 240), 2)\n",
    "            \n",
    "#                 # Change system volume level\n",
    "#                 set_system_volume(volume_level)\n",
    "            \n",
    "    # Show video\n",
    "    cv2.imshow('MediaPipe Hands', image)\n",
    "    # Use ESC key to close webcam\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# Release video capture\n",
    "cam.release()\n",
    "# Memory dump\n",
    "cv2.destroyAllWindows()\n",
    "# fix window not closing bug on macOS 10.15\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d43dfa19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T00:51:03.798521Z",
     "start_time": "2022-05-03T00:51:03.789685Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 1280)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h, w, _ = image.shape\n",
    "h, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "68845bb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T01:15:00.688715Z",
     "start_time": "2022-05-03T01:15:00.638815Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_m/3m0bcc4n6h73gny0bt6_g4x80000gn/T/ipykernel_9543/153568255.py:16: RuntimeWarning: invalid value encountered in arccos\n",
      "  return np.arccos(dot_product / norm)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "441"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# landmark for left hand\n",
    "hand_connections = mp.solutions.holistic.HAND_CONNECTIONS\n",
    "\n",
    "landmark_test = results.multi_hand_landmarks[0].landmark\n",
    "height, width, channel = image.shape \n",
    "\n",
    "landmark_results = []\n",
    "    \n",
    "for _id, landmark in enumerate(landmark_test):\n",
    "    x_coord, y_coord = int(landmark.x * width), int(landmark.y * height)\n",
    "    landmark_results.append([_id, x_coord, y_coord])\n",
    "\n",
    "def get_angle_between_vectors(u, v):\n",
    "    dot_product = np.dot(u, v)\n",
    "    norm = np.linalg.norm(u) * np.linalg.norm(v)\n",
    "    return np.arccos(dot_product / norm)\n",
    "\n",
    "\n",
    "angles_list = []\n",
    "for connection_from in landmark_results:\n",
    "    for connection_to in landmark_results:\n",
    "        angle = get_angle_between_vectors(connection_from, connection_to)\n",
    "        # If the angle is not null we store it else we store 0\n",
    "        if angle:\n",
    "            angles_list.append(angle)\n",
    "        else:\n",
    "            angles_list.append(0)\n",
    "\n",
    "len(angles_list)\n",
    "#np.array(landmark_test).reshape((21, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41d08d5",
   "metadata": {},
   "source": [
    "## Hand Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a7b0b5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T00:10:37.853813Z",
     "start_time": "2022-05-03T00:10:37.844725Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mediapipe.python.solution_base.SolutionOutputs"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class HandModel(object):\n",
    "    \"\"\"\n",
    "    Params\n",
    "        landmarks: List of positions\n",
    "    Args\n",
    "        connections: List of tuples containing the ids of the two landmarks representing a connection\n",
    "        feature_vector: list of length 21*21=441 containing the angles between all connections\n",
    "    \"\"\"\n",
    "    def __init__(self, landmarks: List[float]):\n",
    "        self.connections = mp.solutions.holistic.HAND_CONNECTIONS\n",
    "\n",
    "        landmarks = np.array(landmarks).reshape((21, 3))\n",
    "        self.feature_vector = self._get_feature_vector(landmarks)\n",
    "\n",
    "    def _get_feature_vector(self, landmarks: np.ndarray) -> List[float]:\n",
    "        \"\"\"\n",
    "        Params\n",
    "            landmarks: numpy array of shape (21, 3)\n",
    "        Return\n",
    "            List of length nb_connections * nb_connections containing all the angles between the connections\n",
    "        \"\"\"\n",
    "        connections = self._get_connections_from_landmarks(landmarks)\n",
    "\n",
    "        angles_list = []\n",
    "        for connection_from in connections:\n",
    "            for connection_to in connections:\n",
    "                angle = self._get_angle_between_vectors(connection_from, connection_to)\n",
    "                # If the angle is not null we store it else we store 0\n",
    "                if angle == angle:\n",
    "                    angles_list.append(angle)\n",
    "                else:\n",
    "                    angles_list.append(0)\n",
    "        return angles_list\n",
    "\n",
    "    def _get_connections_from_landmarks(self, landmarks: np.ndarray) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Params\n",
    "            landmarks: numpy array of shape (21, 3)\n",
    "        Return\n",
    "            List of vectors representing hand connections\n",
    "        \"\"\"\n",
    "        return list(\n",
    "            map(\n",
    "                lambda t: landmarks[t[1]] - landmarks[t[0]],\n",
    "                self.connections,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_angle_between_vectors(u: np.ndarray, v: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Params\n",
    "            u, v: 3D vectors representing two connections\n",
    "        Return\n",
    "            Angle between the two vectors\n",
    "        \"\"\"\n",
    "        dot_product = np.dot(u, v)\n",
    "        norm = np.linalg.norm(u) * np.linalg.norm(v)\n",
    "        return np.arccos(dot_product / norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d75eefd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74d1c75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b7a677b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T22:15:14.380674Z",
     "start_time": "2022-04-27T22:15:14.370853Z"
    }
   },
   "source": [
    "### **Sexto passo**  \n",
    "Adicionando o FPS (frames per second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea886849",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) # Mudar o número para corresponder a camera (em geral é 0 ou 1, mas pode ser outro inteiro)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# instanciando a classe\n",
    "hands = mp_hands.Hands() # equivalente a mp.solutions.hands.Hands()\n",
    "\n",
    "# Utilizando o drawing_utils para desenhar as mãos\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "DrawingSpec = mp.solutions.drawing_utils.DrawingSpec\n",
    "\n",
    "tempo_anterior = 0\n",
    "tempo_corrente = 0\n",
    "\n",
    "while True:\n",
    "    sucesso, img = cap.read()\n",
    "    \n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(img_rgb)\n",
    "    \n",
    "    tempo_corrente = time.time()\n",
    "    fps = 1 / (tempo_corrente - tempo_anterior)\n",
    "    \n",
    "    tempo_anterior = tempo_corrente\n",
    "    \n",
    "    if (results.multi_hand_landmarks):\n",
    "        for hand_landmark in results.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(\n",
    "                img,\n",
    "                hand_landmark,\n",
    "                mp_hands.HAND_CONNECTIONS)\n",
    "    \n",
    "    cv2.putText(img, # Onde colocar\n",
    "               str(int(fps)), # convertendo pra string o fps\n",
    "                (10, 70), # posição na imagem\n",
    "                cv2.FONT_HERSHEY_PLAIN, # a fonte\n",
    "                3, # tamanho\n",
    "                (255, 0, 255), # cor\n",
    "                5 # grossura\n",
    "               )\n",
    "    cv2.imshow(\"Imagem\", img)\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06d9e63",
   "metadata": {},
   "source": [
    "### **Setimo passo**  \n",
    "Ok! Conseguimos detectar as mãos e sabemos colocar as conecções entre os landmarks.  \n",
    "Porém para conseguir desenvolver soluções mais complexas, precisamos descobrir qual a posição de cada landmark.\n",
    "Sabemos que o `results.multi_hand_landmarks)` retorna uma lista!  \n",
    "Cada elemento da lista é uma mão! E cada posição da lista refere-se ao landmark.  \n",
    "Então será podemos criar um for loop para tanto desenhar a mão, como extrair os resultados?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71989209",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) # Mudar o número para corresponder a camera (em geral é 0 ou 1, mas pode ser outro inteiro)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# instanciando a classe\n",
    "hands = mp_hands.Hands() # equivalente a mp.solutions.hands.Hands()\n",
    "\n",
    "# Utilizando o drawing_utils para desenhar as mãos\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "DrawingSpec = mp.solutions.drawing_utils.DrawingSpec\n",
    "\n",
    "tempo_anterior = 0\n",
    "tempo_corrente = 0\n",
    "\n",
    "while True:\n",
    "    sucesso, img = cap.read()\n",
    "    \n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(img_rgb)\n",
    "    \n",
    "    tempo_corrente = time.time()\n",
    "    fps = 1 / (tempo_corrente - tempo_anterior)\n",
    "    \n",
    "    tempo_anterior = tempo_corrente\n",
    "    \n",
    "    if (results.multi_hand_landmarks):\n",
    "        for hand_number, hand_landmark in enumerate(results.multi_hand_landmarks):\n",
    "            for _id, landmark in enumerate(hand_landmark.landmark):\n",
    "                print(hand_number, _id, landmark)\n",
    "            \n",
    "            \n",
    "            \n",
    "            mp_draw.draw_landmarks(\n",
    "                img,\n",
    "                hand_landmark,\n",
    "                mp_hands.HAND_CONNECTIONS)\n",
    "    \n",
    "    cv2.putText(img, # Onde colocar\n",
    "               str(int(fps)), # convertendo pra string o fps\n",
    "                (10, 70), # posição na imagem\n",
    "                cv2.FONT_HERSHEY_PLAIN, # a fonte\n",
    "                3, # tamanho\n",
    "                (255, 0, 255), # cor\n",
    "                5 # grossura\n",
    "               )\n",
    "    cv2.imshow(\"Imagem\", img)\n",
    "    cv2.waitKey(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
